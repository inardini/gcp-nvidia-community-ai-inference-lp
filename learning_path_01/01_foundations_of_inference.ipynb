{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "J2PVXjj1qtwg"
      },
      "id": "J2PVXjj1qtwg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Foundations of Inference\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/01_foundations_of_inference.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<p>\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/01_foundations_of_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/01_foundations_of_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/01_foundations_of_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/01_foundations_of_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/01_foundations_of_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>\n",
        "</p>"
      ],
      "metadata": {
        "id": "kNBgd14rP9XZ"
      },
      "id": "kNBgd14rP9XZ"
    },
    {
      "cell_type": "markdown",
      "id": "61edd73b",
      "metadata": {
        "id": "61edd73b"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook introduces the **fundamental concepts of model inference** — how a trained model takes an input and produces an output.  \n",
        "\n",
        "We will build a step-by-step understanding of the inference pipeline:\n",
        "\n",
        "- **CPU vs GPU inference** — showing why hardware acceleration matters.  \n",
        "- **Inference pipeline** — preprocessing, batching & padding, forward pass, decoding, and postprocessing.  \n",
        "\n",
        "The focus here is on clarity and education: we use a small model so you can see the mechanics of inference clearly before moving to larger and more optimized setups."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6f116d-47ea-4b21-ae08-36a604858da2",
      "metadata": {
        "id": "da6f116d-47ea-4b21-ae08-36a604858da2"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "**Before you begin**, make sure you have:\n",
        "\n",
        "- An NVIDIA GPU environment\n",
        "- Access to the gated [Gemma 3 270M](https://huggingface.co/google/gemma-3-270m) or [Gemma 3 270M Instruction-Tuned](https://huggingface.co/google/gemma-3-270m-it) model on Hugging Face\n",
        "- Your Hugging Face [access token](https://huggingface.co/settings/token)\n",
        "\n",
        "Let's test which GPUs are avaialable in our system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9ef8f2-9645-4cf5-8f68-5f5c33c69317",
      "metadata": {
        "id": "ed9ef8f2-9645-4cf5-8f68-5f5c33c69317"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e21d5505-399a-4a06-a8a7-11489bfdf8f3",
      "metadata": {
        "id": "e21d5505-399a-4a06-a8a7-11489bfdf8f3"
      },
      "source": [
        "### Authenticating with Hugging Face\n",
        "\n",
        "To download the model from Hugging Face, you’ll need to enter your personal access token.\n",
        "\n",
        "The cell below provides a simple interface to enter and save your token securely. It will be cached locally, so you only need to do this once per environment.\n",
        "\n",
        "➡️ You can find or generate your token at: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "Once saved, the token will allow seamless access to gated models like `google/gemma-3-270m`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4136881a-daff-45ed-a56e-0b80f056cd9c",
      "metadata": {
        "id": "4136881a-daff-45ed-a56e-0b80f056cd9c"
      },
      "outputs": [],
      "source": [
        "# ⬇️ Run this cell once\n",
        "from ipywidgets import Password, Button, HBox, Output\n",
        "import os, pathlib\n",
        "import sys\n",
        "\n",
        "from huggingface_hub import HfFolder, whoami\n",
        "\n",
        "# ---- UI widgets ----\n",
        "token_box = Password(\n",
        "    description=\"HF Token:\",\n",
        "    placeholder=\"paste your Hugging Face token here\",\n",
        "    layout={\"width\": \"450px\"},\n",
        ")\n",
        "save_btn = Button(description=\"Save\", button_style=\"success\")\n",
        "out = Output()\n",
        "\n",
        "# ---- Callback ----\n",
        "def save_token(_):\n",
        "    out.clear_output()\n",
        "    token = token_box.value.strip()\n",
        "    with out:\n",
        "        if not token:\n",
        "            print(\"❌ No token entered.\")\n",
        "            return\n",
        "        # Persist token\n",
        "        HfFolder.save_token(token)                 # writes to ~/.cache/huggingface/token\n",
        "        os.environ[\"HF_TOKEN\"] = token             # current kernel env (optional)\n",
        "        # Sanity-check who we are\n",
        "        try:\n",
        "            user = whoami(token)[\"name\"]\n",
        "            print(f\"✅ Token saved. Logged in as: {user}\")\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ Token saved, but user lookup failed:\", e)\n",
        "\n",
        "save_btn.on_click(save_token)\n",
        "\n",
        "display(HBox([token_box, save_btn]), out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea25a290-ffc8-406f-a844-59b68d2c2d5d",
      "metadata": {
        "id": "ea25a290-ffc8-406f-a844-59b68d2c2d5d"
      },
      "source": [
        "## Training vs inference\n",
        "\n",
        "In machine learning, the lifecycle of a model can be divided into two primary phases: **training** and **inference**. During training, the model learns to identify patterns and relationships within the data. This process involves a **forward pass**, where input data is propagated through the network to generate predictions, followed by a **backward pass**, in which gradients of a loss function with respect to the model parameters are computed. An **optimizer** then uses these gradients to adjust the model parameters iteratively, minimizing the loss and improving the model’s predictive accuracy.\n",
        "\n",
        "Once training is complete, the model enters the inference phase, where it is used to make predictions on new, unseen data. Inference is simpler than training because it involves only the forward pass; the model parameters are fixed and no learning occurs.\n",
        "\n",
        "*Inference is the process of using a trained model to generate outputs for new inputs without any learning or weight updates.*\n",
        "\n",
        "For large language models (LLMs), this means:\n",
        "\n",
        "- Receiving a prompt (a string of text).\n",
        "- Computing the most likely next token(s).\n",
        "- Repeating the process token-by-token to generate a full response.\n",
        "\n",
        "Inference is the core of every production LLM system — whether you’re building a chatbot, writing assistant, summarizer, or anything else."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20a668c2-1952-4bcd-8e83-77810f321783",
      "metadata": {
        "id": "20a668c2-1952-4bcd-8e83-77810f321783"
      },
      "source": [
        "### Why do we need GPUs for inference\n",
        "\n",
        "You may have heard that we need a GPU to run AI inference. Why is so? To answer this question let's run a simple experiment.\n",
        "\n",
        "Let's first download our model. We wil be usung the model [Gemma 3 270M Instruction-Tuned](https://huggingface.co/google/gemma-3-270m-it) by Google. Before you continue, make sure to open the [model page](https://huggingface.co/google/gemma-3-270m-it) and accept the terms of usage agreement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61363f28-3614-4cd2-8664-0c40914ce8b3",
      "metadata": {
        "id": "61363f28-3614-4cd2-8664-0c40914ce8b3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time, numpy as np\n",
        "\n",
        "\n",
        "# Helper function to measure inference time\n",
        "def timed_generate(model, tokenizer, prompts, device, max_new_tokens=64, runs=5):\n",
        "    enc = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
        "    times = []\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(runs):\n",
        "            t0 = time.perf_counter()\n",
        "            _ = model.generate(**enc, max_new_tokens=max_new_tokens)\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.synchronize()\n",
        "            times.append(time.perf_counter() - t0)\n",
        "    lat = np.array(times)\n",
        "    return lat\n",
        "\n",
        "MODEL_ID = \"google/gemma-3-270m-it\"\n",
        "\n",
        "# Make sure you've entered your Hugging Face token above\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "# Ensure we have a pad token for batching\n",
        "if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model on CPU\n",
        "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    dtype=torch.float32,\n",
        "    device_map={\"\": \"cpu\"}\n",
        ").eval()\n",
        "\n",
        "# Load model directly on GPU\n",
        "model_gpu = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
        "    device_map=\"cuda\"   # puts all weights on the GPU\n",
        ").eval()\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce68b075-e953-441d-b49a-8fb704289886",
      "metadata": {
        "id": "ce68b075-e953-441d-b49a-8fb704289886"
      },
      "source": [
        "#### CPU baseline\n",
        "\n",
        "Let's start with the CPU baseline. We will execute our inference several times using a simple prompt and measure execution times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d13300-57ec-4ad7-b59b-a8a175900c96",
      "metadata": {
        "id": "30d13300-57ec-4ad7-b59b-a8a175900c96"
      },
      "outputs": [],
      "source": [
        "prompt = \"Explain inference vs training in one sentence.\"\n",
        "batch = [prompt]\n",
        "\n",
        "cpu_lat = timed_generate(model_cpu, tokenizer, batch, device=\"cpu\")\n",
        "print(\"CPU execution times:\", cpu_lat)\n",
        "print(\"CPU median execution time:\", np.median(cpu_lat), \"s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5780b3dd-5e28-4f8b-8923-fcd0bc0030ab",
      "metadata": {
        "id": "5780b3dd-5e28-4f8b-8923-fcd0bc0030ab"
      },
      "source": [
        "#### Executing on GPU\n",
        "\n",
        "Now we will do the same procedure, but on the GPU. We will be using the same prompt and batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "056bb003-dc49-4b88-92da-e7be1586e58d",
      "metadata": {
        "id": "056bb003-dc49-4b88-92da-e7be1586e58d"
      },
      "outputs": [],
      "source": [
        "prompt = \"Explain inference vs training in one sentence.\"\n",
        "batch = [prompt]\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_lat = timed_generate(model_gpu, tokenizer, batch, device=\"cuda\")\n",
        "    print(\"GPU execution times:\", gpu_lat)\n",
        "    print(\"GPU median execution time:\", np.median(gpu_lat), \"s\")\n",
        "else:\n",
        "    print(\"No CUDA device available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54cfd659-657b-4bae-a548-3c45aa925927",
      "metadata": {
        "id": "54cfd659-657b-4bae-a548-3c45aa925927"
      },
      "source": [
        "#### Side-by-side latency plot\n",
        "\n",
        "**Latency** is the time it takes for a system to return a result after receiving a request.\n",
        "\n",
        "In inference, it is measured as the **time from when a prompt is submitted until the model produces a complete output (or the first token, if streaming)**.\n",
        "\n",
        "- Lower latency → faster response → better user experience.  \n",
        "- Higher latency → longer wait times → degraded user experience.\n",
        "\n",
        "So, in our experiments, latency is the execution time we've measured.\n",
        "\n",
        "Let's compare CPU and GPU latencies side-by-side!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62a0a43b-b1fd-4915-ae3d-6e16957201b4",
      "metadata": {
        "id": "62a0a43b-b1fd-4915-ae3d-6e16957201b4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.boxplot([cpu_lat, gpu_lat], labels=[\"CPU\",\"GPU\"])\n",
        "plt.ylabel(\"Latency (s)\")\n",
        "plt.title(\"CPU vs GPU Latency — Gemma 3 270M\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c728ab1f-b488-4e5f-a9c9-f6f891e669ac",
      "metadata": {
        "id": "c728ab1f-b488-4e5f-a9c9-f6f891e669ac"
      },
      "source": [
        "What you should be able to see here is that on average GPU latencies are lower than CPU latencies. Some outliers are possible, but what matters is the median value.\n",
        "\n",
        "Note that Gemma 3 270M is a lightweight model and this difference would be more drammatic on bigger models.\n",
        "\n",
        "#### Why is GPU inference faster?\n",
        "\n",
        "GPUs are designed to handle thousands of operations in parallel, while CPUs typically focus on fewer tasks in sequence.  \n",
        "\n",
        "During inference, LLMs perform a huge number of matrix multiplications — operations that map perfectly onto the GPU’s parallel architecture.  \n",
        "\n",
        "- **Parallelism:** A GPU has thousands of cores that can process many elements of a matrix at once, whereas a CPU has far fewer cores optimized for general tasks.  \n",
        "- **Throughput:** This parallelism lets GPUs achieve much higher throughput, meaning they can process larger batches of tokens or requests efficiently.  \n",
        "- **Specialized hardware:** Modern GPUs also include optimized libraries and hardware features (like Tensor Cores) that accelerate deep learning operations far beyond what CPUs can achieve.  \n",
        "\n",
        "The result is that the same model produces responses **orders of magnitude faster** on a GPU than on a CPU — which directly improves user experience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f6f125-cae8-4110-9099-e22aadf610e4",
      "metadata": {
        "id": "00f6f125-cae8-4110-9099-e22aadf610e4"
      },
      "source": [
        "## Inference pipeline and its components\n",
        "\n",
        "When we talk about *inference*, we mean the process of turning an input prompt into an output sequence.  \n",
        "\n",
        "The pipeline has four main components:\n",
        "\n",
        "1. **Preprocessing** – tokenize text into IDs the model understands.  \n",
        "2. **Batching & padding** – pack multiple requests together for efficiency.  \n",
        "3. **Forward pass + decoding** – model computes logits, selects tokens step by step (with KV-cache for efficiency).  \n",
        "4. **Postprocessing** – convert token IDs back into human-readable text.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebe492e6-1f3a-41c8-b052-2899b5218e73",
      "metadata": {
        "id": "ebe492e6-1f3a-41c8-b052-2899b5218e73"
      },
      "source": [
        "### Preprocessing (tokenization) and batching & padding\n",
        "\n",
        "In this example, we pass **two prompts of very different lengths** to the tokenizer.  \n",
        "\n",
        "Because models expect inputs to be rectangular tensors, all sequences in a batch must have the **same length**.  \n",
        "\n",
        "- The **short prompt** `\"Hi\"` becomes just a couple of tokens.  \n",
        "- The **long prompt** expands to many tokens.  \n",
        "- To align them into the same tensor, the tokenizer **pads the shorter input** with zeros at the end.  \n",
        "\n",
        "The result:  \n",
        "- `input_ids` → contains both the real token IDs and padding tokens (0).  \n",
        "- `attention_mask` → uses `1` to mark real tokens and `0` to mark padding positions, so the model ignores padding during computation.  \n",
        "\n",
        "This way, both prompts can be processed **in parallel** on the GPU, even though they originally had different lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d5d2131-33db-4d77-9f6a-78dba2054a38",
      "metadata": {
        "id": "8d5d2131-33db-4d77-9f6a-78dba2054a38"
      },
      "outputs": [],
      "source": [
        "# Prompts (batch of 2)\n",
        "prompts = [\n",
        "    \"Hi\",\n",
        "    \"Explain in detail what AI model training means and why it is important.\"\n",
        "]\n",
        "\n",
        "# Tokenize on CPU (cheap)\n",
        "encodings = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "print(\"input_ids shape:\", encodings[\"input_ids\"].shape)\n",
        "print(\"attention_mask shape:\", encodings[\"attention_mask\"].shape)\n",
        "\n",
        "print(\"Batch size:\", encodings[\"input_ids\"].shape[0])\n",
        "print(\"Sequence length (after padding):\", encodings[\"input_ids\"].shape[1])\n",
        "\n",
        "# Inspect the raw values\n",
        "print(\"\\ninput_ids[0]:\", encodings[\"input_ids\"][0].tolist())\n",
        "print(\"attention_mask[0]:\", encodings[\"attention_mask\"][0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6279a487-9d40-47ac-b635-dcfb977f0932",
      "metadata": {
        "id": "6279a487-9d40-47ac-b635-dcfb977f0932"
      },
      "source": [
        "### Forward pass (logits only)\n",
        "\n",
        "Here we send the tokenized batch to the **GPU** and run a **forward pass** through the model **without** generating text.\n",
        "\n",
        "- `torch.inference_mode()` disables gradient tracking and optimizer state, which is what we want at inference time (lower overhead, faster).\n",
        "- The model returns **logits**: raw, unnormalized scores for every vocabulary token at **each position** in each sequence.\n",
        "- Shape check:\n",
        "  - **batch** = number of prompts in the batch\n",
        "  - **seq_len** = length of each (padded) input sequence\n",
        "  - **vocab_size** = number of tokens the model can output\n",
        "  - So `logits.shape == (batch, seq_len, vocab_size)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f97182-2810-422e-8789-dbaeab6b0782",
      "metadata": {
        "id": "04f97182-2810-422e-8789-dbaeab6b0782"
      },
      "outputs": [],
      "source": [
        "# Move tensors to GPU\n",
        "enc = {k: v.to(\"cuda\") for k, v in encodings.items()}\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs = model_gpu(**enc)           # forward only (no generation)\n",
        "logits = outputs.logits                  # (batch, seq_len, vocab_size)\n",
        "print(\"Logits shape:\", logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b29d93f6-3be0-4125-bc80-89f70a68a32c",
      "metadata": {
        "id": "b29d93f6-3be0-4125-bc80-89f70a68a32c"
      },
      "source": [
        "What this means:\n",
        "\n",
        "- For each position in the input, the model predicts a distribution over the next token.  \n",
        "- We haven’t applied `softmax` yet (that would convert logits to probabilities).\n",
        "- We also haven’t **decoded** anything—this step only computes scores; the **decoder** will pick actual next tokens in the following step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "473d6b15-a6eb-4ec5-a8ff-65bd0d0bb26d",
      "metadata": {
        "id": "473d6b15-a6eb-4ec5-a8ff-65bd0d0bb26d"
      },
      "source": [
        "### Decoding step-by-step\n",
        "\n",
        "This loop takes the **logits from the forward pass** and turns them into actual text, one token at a time.\n",
        "\n",
        "**Initialization**\n",
        "   - We start with the original input tokens (`input_ids`) and an attention mask.  \n",
        "   - `generated` holds the full sequence as it grows with each new token.  \n",
        "\n",
        "**Loop**\n",
        "   - For each decoding step:\n",
        "     - We pass the **entire sequence** back through the model.  \n",
        "     - The model outputs logits for every position, but we only care about the **last position** (`out.logits[:, -1, :]`).  \n",
        "     - Using `argmax`, we greedily pick the token with the highest score as the next token.  \n",
        "   - We then **append** this token to `generated` and update the mask so the model counts it as a real token in the next step.  \n",
        "\n",
        "**Naive decoding (no cache)**\n",
        "   - Notice that at each step we recompute the whole sequence from scratch.  \n",
        "   - This is simple to understand but computationally inefficient, because earlier computations are repeated over and over.  \n",
        "   - In the future learning paths, we’ll learn about **KV cache**, which avoids this repetition and makes decoding much faster.  \n",
        "\n",
        "At the end of the loop, `generated` contains the full sequence of input + generated tokens, and the model has extended the text step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778dd3b5-4687-4709-ae75-7f227bdb5c58",
      "metadata": {
        "id": "778dd3b5-4687-4709-ae75-7f227bdb5c58"
      },
      "outputs": [],
      "source": [
        "# Start from encoded inputs on GPU\n",
        "generated = enc[\"input_ids\"].clone()\n",
        "mask = enc[\"attention_mask\"].clone()\n",
        "\n",
        "STEPS = 40  # how many new tokens to generate\n",
        "\n",
        "for _ in range(STEPS):\n",
        "    with torch.inference_mode():\n",
        "        out = model_gpu(input_ids=generated, attention_mask=mask)\n",
        "        next_id = out.logits[:, -1, :].argmax(dim=-1)  # greedy pick (no sampling)\n",
        "\n",
        "    # Append new token and update attention mask\n",
        "    generated = torch.cat([generated, next_id.unsqueeze(-1)], dim=-1)\n",
        "    mask = torch.cat([mask, torch.ones_like(next_id).unsqueeze(-1)], dim=-1)\n",
        "\n",
        "print(\"Generated token IDs:\", generated)\n",
        "print(\"Mask:\", mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fbd7e1f-f92c-42ee-857b-824b87536e3f",
      "metadata": {
        "id": "1fbd7e1f-f92c-42ee-857b-824b87536e3f"
      },
      "source": [
        "### Postprocessing (detokenize to text)\n",
        "\n",
        "The model works with token IDs internally, but we need to turn those back into readable text.  \n",
        "\n",
        "- `tokenizer.batch_decode` converts each sequence of IDs into a string.  \n",
        "- `skip_special_tokens=True` removes tokens like `<pad>` or `<eos>` that are only used internally.  \n",
        "- The result is the **final human-readable output** for each prompt in the batch.  \n",
        "\n",
        "This step completes the inference pipeline: **text → tokens → model → new tokens → text**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d6ee659-c97b-4e26-bb26-868041668cf1",
      "metadata": {
        "id": "6d6ee659-c97b-4e26-bb26-868041668cf1"
      },
      "outputs": [],
      "source": [
        "decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
        "for i, txt in enumerate(decoded, 1):\n",
        "    print(f\"Output {i}:\\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "353dd0df-f9a2-4709-a93b-604514bfefe3",
      "metadata": {
        "id": "353dd0df-f9a2-4709-a93b-604514bfefe3"
      },
      "source": [
        "### Extending inference to other modalities\n",
        "\n",
        "The inference pipeline we explored with text models — preprocessing, batching & padding, forward pass, decoding, and postprocessing — is not unique to language. The same structure appears in every domain where machine learning models are used, though the exact details of each stage differ by modality.\n",
        "\n",
        "**Preprocessing:**  \n",
        "\n",
        "For text, preprocessing involves tokenization — converting words and sentences into numeric token IDs. In other modalities, this step adapts inputs into the numerical format the model was trained on:\n",
        "\n",
        "- **Images:** raw pixel arrays may be resized, normalized, and sometimes patched into chunks before entering a vision transformer.  \n",
        "- **Audio:** waveforms are often converted into spectrograms or mel-frequency features.  \n",
        "- **Video:** sequences of frames are sampled, resized, and sometimes combined with audio features.  \n",
        "\n",
        "Despite differences, the goal is always the same: transform raw human data into the structured tensors that models can process.\n",
        "\n",
        "**Batching & Padding:**  \n",
        "\n",
        "Just like with text sequences of different lengths, inputs from other domains often vary in size.  \n",
        "\n",
        "- **Images:** can be padded or cropped to a consistent resolution.  \n",
        "- **Audio:** recordings of different durations are padded with silence to align in length.  \n",
        "- **Video:** clips may be padded with empty frames to create uniform sequences.  \n",
        "\n",
        "Batching allows multiple inputs to be processed in parallel, making efficient use of hardware accelerators.\n",
        "\n",
        "**Forward Pass & Decoding:**  \n",
        "\n",
        "The model’s forward pass computes predictions from the preprocessed input:\n",
        "\n",
        "- In **image classification**, logits correspond to class scores (e.g., cat vs. dog).  \n",
        "- In **speech-to-text**, logits represent the likelihood of characters or subword tokens, which are then decoded into text.  \n",
        "- In **text-to-image**, the model predicts pixel or latent representations step by step until an image emerges.  \n",
        "\n",
        "Decoding strategies differ, but the principle is the same: the model outputs raw scores, and decoding selects meaningful outputs from those scores.\n",
        "\n",
        "**Postprocessing:**  \n",
        "\n",
        "The final step turns raw outputs into a usable format:  \n",
        "\n",
        "- Class scores become human-readable labels.  \n",
        "- Token IDs become text.  \n",
        "- Generated pixel arrays become images.  \n",
        "- Predicted audio waveforms are converted back into playable sound.  \n",
        "\n",
        "#### Key Insight\n",
        "\n",
        "Regardless of modality, inference always follows the same rhythm: **convert inputs into tensors → run the model → convert outputs back into a human-usable form**. Once learners understand the text pipeline, they can map the same concepts to images, audio, video, or multimodal systems with only small adjustments to the preprocessing and postprocessing stages."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3682e8d3-7488-4cdf-a941-b73d9cfb4dfb",
      "metadata": {
        "id": "3682e8d3-7488-4cdf-a941-b73d9cfb4dfb"
      },
      "source": [
        "## Stop this notebook\n",
        "\n",
        "Make sure to restart the jupyter kernel before proceeding with next excercizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4144d63-4892-4c8d-8fbf-1a39a2a92f2a",
      "metadata": {
        "id": "f4144d63-4892-4c8d-8fbf-1a39a2a92f2a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os._exit(0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}