{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c92c51-3635-4e69-a96c-f9107211d983",
   "metadata": {},
   "source": [
    "# Inference Metrics: Latency, Throughput, and UX\n",
    "\n",
    "This notebook is a hands-on introduction to the key performance aspects of running inference with Large Language Models (LLMs).\n",
    "\n",
    "You’ll learn:\n",
    "\n",
    "- What **latency** and **throughput** mean in the context of LLMs.\n",
    "- Why these metrics often trade off against each other. \n",
    "- How different parameters (like batch size, prompt length, sampling strategy) affect performance.\n",
    "- How to measure and visualize **p50 vs p90 latency**, **first-token vs total latency**, and find the \"sweet spot\".\n",
    "- What this means for real-world **user experience**.\n",
    "\n",
    "By the end of this notebook, you'll not only be able to benchmark an LLM — you'll know what the numbers actually mean and how to tune them for real applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5b987-c231-4acb-8793-fb735869ffb0",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "**Before you begin**, make sure you have:\n",
    "\n",
    "- An NVIDIA GPU environment\n",
    "- Your Hugging Face [access token](https://huggingface.co/settings/token)\n",
    "\n",
    "Let's test which GPUs are avaialable in our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1995f8c-23c9-4c65-b470-40b8d3e4d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f706b-d849-42ca-beff-5d5d43cf5c52",
   "metadata": {},
   "source": [
    "### Authenticating with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ebed9c-8299-43cd-9f1b-d154dcd9cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⬇️ Run this cell once\n",
    "from ipywidgets import Password, Button, HBox, Output\n",
    "import os, pathlib\n",
    "import sys\n",
    "\n",
    "from huggingface_hub import HfFolder, whoami\n",
    "\n",
    "# ---- UI widgets ----\n",
    "token_box = Password(\n",
    "    description=\"HF Token:\",\n",
    "    placeholder=\"paste your Hugging Face token here\",\n",
    "    layout={\"width\": \"450px\"},\n",
    ")\n",
    "save_btn = Button(description=\"Save\", button_style=\"success\")\n",
    "out = Output()\n",
    "\n",
    "# ---- Callback ----\n",
    "def save_token(_):\n",
    "    out.clear_output()\n",
    "    token = token_box.value.strip()\n",
    "    with out:\n",
    "        if not token:\n",
    "            print(\"❌ No token entered.\")\n",
    "            return\n",
    "        # Persist token\n",
    "        HfFolder.save_token(token)                 # writes to ~/.cache/huggingface/token\n",
    "        os.environ[\"HF_TOKEN\"] = token             # current kernel env (optional)\n",
    "        # Sanity-check who we are\n",
    "        try:\n",
    "            user = whoami(token)[\"name\"]\n",
    "            print(f\"✅ Token saved. Logged in as: {user}\")\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Token saved, but user lookup failed:\", e)\n",
    "\n",
    "save_btn.on_click(save_token)\n",
    "\n",
    "display(HBox([token_box, save_btn]), out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0b5dd-62a3-4704-9304-0a8f1ca1645a",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253273e9-d8d1-47d3-993c-bcfd546acc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-10 00:15:00 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-10 00:15:03 [utils.py:326] non-default args: {'model': 'nvidia/NVIDIA-Nemotron-Nano-9B-v2', 'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 131072, 'max_num_seqs': 64, 'disable_log_stats': True, 'mamba_ssm_cache_dtype': 'float32'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-10 00:15:15 [__init__.py:711] Resolved architecture: NemotronHForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-10 00:15:15 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 09-10 00:15:15 [__init__.py:1750] Using max model len 131072\n",
      "INFO 09-10 00:15:15 [arg_utils.py:1781] Mamba is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "WARNING 09-10 00:15:15 [arg_utils.py:1544] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
      "INFO 09-10 00:15:15 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 09-10 00:15:15 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='nvidia/NVIDIA-Nemotron-Nano-9B-v2', speculative_config=None, tokenizer='nvidia/NVIDIA-Nemotron-Nano-9B-v2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=nvidia/NVIDIA-Nemotron-Nano-9B-v2, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{\"enable_fusion\":false,\"enable_noop\":false},\"max_capture_size\":64,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 09-10 00:15:19 [cuda.py:436] Using Flash Attention backend.\n",
      "INFO 09-10 00:15:19 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-10 00:15:19 [model_runner.py:1080] Starting to load model nvidia/NVIDIA-Nemotron-Nano-9B-v2...\n",
      "INFO 09-10 00:15:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e016db92e64ce5995bef08e6438eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-10 00:15:33 [default_loader.py:262] Loading weights took 11.97 seconds\n",
      "INFO 09-10 00:15:33 [model_runner.py:1112] Model loading took 16.5557 GiB and 12.859213 seconds\n",
      "INFO 09-10 00:15:42 [worker.py:295] Memory profiling takes 8.32 seconds\n",
      "INFO 09-10 00:15:42 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.41GiB) x gpu_memory_utilization (0.90) = 42.67GiB\n",
      "INFO 09-10 00:15:42 [worker.py:295] model weights take 16.56GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 9.44GiB; the rest of the memory reserved for KV Cache is 16.61GiB.\n",
      "INFO 09-10 00:15:42 [executor_base.py:114] # cuda blocks: 68042, # CPU blocks: 16384\n",
      "INFO 09-10 00:15:42 [executor_base.py:119] Maximum concurrency for 131072 tokens per request: 8.31x\n",
      "INFO 09-10 00:15:44 [model_runner.py:1383] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d2c08ffad84787a8e1c28870a13d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-10 00:15:53 [model_runner.py:1535] Graph capturing finished in 9 secs, took 0.14 GiB\n",
      "INFO 09-10 00:15:53 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 19.99 seconds\n",
      "INFO 09-10 00:15:55 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "\n",
    "MODEL_ID = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "tp = int(os.getenv(\"TP_SIZE\", \"1\"))  # set >1 if you want tensor parallel across GPUs\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_ID,\n",
    "    dtype=\"float16\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=tp,\n",
    "    max_num_seqs=64,\n",
    "    max_model_len=131072,\n",
    "    mamba_ssm_cache_dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ed929-eaf2-4f55-999b-09153059af67",
   "metadata": {},
   "source": [
    "### Sampling parameters\n",
    "\n",
    "When generating text, the model can either:\n",
    "\n",
    "- **Always pick the highest-probability token** (greedy decoding — fast and deterministic)\n",
    "- **Sample from the probability distribution** over possible next tokens — which adds variety and creativity\n",
    "\n",
    "We use the following parameters to control that behavior:\n",
    "\n",
    "- `temperature = 0.3`: Controls randomness. Lower values → more confident, deterministic outputs. Higher values → more diverse, sometimes erratic responses.\n",
    "- `top_p = 0.9`: Enables **nucleus sampling** — the model samples only from the top tokens that together make up 90% of the probability mass. Balances diversity and coherence.\n",
    "- `max_tokens=512`: Controls maximum number of tokens to generate.\n",
    "\n",
    "Together, these settings ensure the output is diverse but not chaotic — a good balance for most use cases.\n",
    "\n",
    "When running experiments, you can try changing the prompts and sampling values below to see how the model’s behavior changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c9576-1b59-4c1e-a87a-ec3decb7ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = SamplingParams(\n",
    "    max_tokens=512, temperature=0.3, top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bcd299-8b1a-4185-b6bb-b0898d195fc7",
   "metadata": {},
   "source": [
    "### Convertng user prompts to chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3faaf21-9ab0-4ce4-bc13-9bcb6f7c4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chat(messages):\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "      {\"role\": \"system\", \"content\": \"/no_think\"},   # or \"/think\"\n",
    "      {\"role\": \"user\",   \"content\": \"Write a haiku about GPUs\"}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1e93d-765a-444b-a2d5-15fb32cfd46a",
   "metadata": {},
   "source": [
    "## Executing inference with Nemotron Nano 9b v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d1243-856a-48d0-b2fb-92c6281643a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat prompt with reasoning OFF\n",
    "messages = [\n",
    "    {\"role\":\"system\", \"content\": \"/no_think\"},\n",
    "    {\"role\":\"user\",   \"content\": \"Explain KV cache in one paragraph.\"}\n",
    "]\n",
    "chat_prompt = to_chat(messages)\n",
    "\n",
    "outs = llm.generate([chat_prompt], sampling)\n",
    "for i, o in enumerate(outs, 1):\n",
    "    print(f\"=== Output {i} ===\\n{o.outputs[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94030624-e454-4319-8b87-a4cc6033a170",
   "metadata": {},
   "source": [
    "## Performance experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149c7f0-7c42-44dc-ba2e-6a94d9e9c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nemotron Nano-9B-v2 benchmarking helpers (reasoning OFF)\n",
    "# --- Imports & reproducibility --- \n",
    "import os, time, random, math, json, itertools, statistics, gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import SamplingParams  # llm must already exist in your session\n",
    "\n",
    "import os, logging\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"                 # tqdm\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\" # huggingface-hub\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"   # Transformers text logs\n",
    "\n",
    "# Optional: quiet vLLM info logs too\n",
    "logging.getLogger(\"vllm\").setLevel(logging.ERROR)\n",
    "try:\n",
    "    from transformers.utils.logging import set_verbosity_error\n",
    "    set_verbosity_error()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = False  # stabilizes variance a bit\n",
    "\n",
    "\n",
    "def to_chat_no_think(user_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Format a single-turn chat with reasoning explicitly OFF\n",
    "    using the model's chat template.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
    "        {\"role\": \"user\",   \"content\": user_text},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "# --- Timed generation with optional batching --- \n",
    "import io, contextlib\n",
    "\n",
    "def timed_generate(prompts, sampling_params, as_chat=True, return_counts=True, quiet=True):\n",
    "    \"\"\"Return (elapsed_s, total_gen_tokens, [counts], outputs).\"\"\"\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    wrapped = [to_chat_no_think(p) for p in prompts] if as_chat else prompts\n",
    "\n",
    "    # define the actual call with tqdm disabled\n",
    "    def _call():\n",
    "        return llm.generate(wrapped, sampling_params, use_tqdm=False)\n",
    "\n",
    "    # time + suppress stdout/stderr if quiet\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    if quiet:\n",
    "        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
    "            outputs = _call()\n",
    "    else:\n",
    "        outputs = _call()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    # per-sequence counts\n",
    "    counts = []\n",
    "    for o in outputs:\n",
    "        cand = o.outputs[0]\n",
    "        if getattr(cand, \"token_ids\", None) is not None:\n",
    "            cnt = len(cand.token_ids)\n",
    "        elif getattr(cand, \"token_count\", None) is not None:\n",
    "            cnt = int(cand.token_count)\n",
    "        else:\n",
    "            cnt = len(tokenizer.encode(cand.text, add_special_tokens=False))\n",
    "        counts.append(cnt)\n",
    "\n",
    "    total = sum(counts)\n",
    "    if return_counts:\n",
    "        return elapsed, total, counts, outputs\n",
    "    return elapsed, total, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488963f5-0e20-4299-b4a9-2511b8fec342",
   "metadata": {},
   "source": [
    "### Baseline latency and throughput\n",
    "\n",
    "Let’s establish a **baseline** for how our model performs with a single prompt under typical sampling settings.\n",
    "\n",
    "This test measures:\n",
    "\n",
    "- **Latency**: the total time it takes to generate a complete response\n",
    "- **Throughput**: the number of tokens generated per second\n",
    "\n",
    "What we’re doing here:\n",
    "\n",
    "- Use a **single input prompt** (feel free to change it!)\n",
    "- Generate up to `256` tokens using greedy sampling (`temperature=0.0`, `top_p=1.0`)\n",
    "- Call `timed_generate()` to:\n",
    "  - time the entire inference run\n",
    "  - count how many tokens were actually generated\n",
    "- Print the generated response\n",
    "\n",
    "This gives us a **reference point** to compare against later experiments where we vary different factors.\n",
    "\n",
    "> This is also the simplest “real-world” case: one user, one prompt, one answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e628f08-d4d9-4371-8972-bac2f357532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage --- \n",
    "# Greedy for latency/throughput measurement\n",
    "greedy = SamplingParams(max_tokens=256, temperature=0.0, top_p=1.0)\n",
    "\n",
    "batch = [\n",
    "    \"The future of AI is\",\n",
    "    \"Explain KV cache eviction in ~4 lines.\",\n",
    "    \"INT8 vs FP16 for LLMs—give 5 concise tradeoffs.\",\n",
    "    \"Why does batching help prefill more than decode?\"\n",
    "] * 4  # 16 requests to exercise scheduling\n",
    "\n",
    "elapsed, gen_toks, counts, outs  = timed_generate(batch, greedy, as_chat=True, return_counts=True)\n",
    "tok_s = gen_toks / elapsed if elapsed > 0 else float(\"nan\")\n",
    "print(f\"Requests: {len(batch)} | Gen tokens: {gen_toks} | Time: {elapsed:.2f}s | {tok_s:.1f} tok/s\")\n",
    "\n",
    "# Peek at first few outputs\n",
    "for i, o in enumerate(outs[:3], 1):\n",
    "    print(f\"\\n[{i}] {o.outputs[0].text.strip()[:600]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8d3e9-b348-49eb-b289-320f2e241002",
   "metadata": {},
   "source": [
    "### Batch size sweep\n",
    "\n",
    "Now we’re going to benchmark how inference performance changes with **different batch sizes** — that is, how many prompts we process in a single inference.\n",
    "\n",
    "This experiment helps us understand the **tradeoff between throughput and latency**, and find the batch size that balances performance and responsiveness.\n",
    "\n",
    "For each batch size in:\n",
    "\n",
    "```python\n",
    "[1, 8, 32, 64, 128, 256]\n",
    "```\n",
    "\n",
    "we run several repetitions and record:\n",
    "\n",
    "- Sustained throughput (capacity)\n",
    "- p50 / p90 total latency and TTFT\n",
    "- p50 / p90 throughput (from sec/token)\n",
    "- p50 / p90 ITL (ms/token, excludes first token per active sequence)\n",
    "\n",
    "> These runs can take a few minutes, especially at large batch sizes — start executing the cell and grab a coffee!\n",
    "\n",
    "We will be saving the results in the file `batch_benchmark.csv` so you can reuse or visualize the data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfae0d7-5231-4c84-a3aa-8c4cefd0d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nemotron Nano-9B-v2 batch benchmark (reasoning OFF) — meaningful metrics only\n",
    "\n",
    "import gc, numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from vllm import SamplingParams\n",
    "\n",
    "BATCH_SIZES     = [1, 8, 32, 64, 128, 256]\n",
    "RUNS_PER_SIZE   = 10\n",
    "WARMUP_RUNS     = 2                      # drop warmups (JIT/capture)\n",
    "MAX_TOKENS_FULL = 128\n",
    "USE_GREEDY_SAMPLING = False\n",
    "TEMPERATURE     = 0.7\n",
    "TOP_P           = 0.9\n",
    "\n",
    "PROMPT_POOL = [\n",
    "    \"The future of AI is\",\n",
    "    \"Summarize in 1–2 sentences: why batching helps decode less than prefill.\",\n",
    "    \"Give 5 bullets on KV cache eviction strategies.\",\n",
    "    \"Explain attention vs. Mamba-2 in 3 concise points.\",\n",
    "    \"Write a 2-sentence TL;DR on FP16 vs. BF16 tradeoffs.\",\n",
    "    \"List 3 pitfalls when benchmarking LLMs.\"\n",
    "]\n",
    "\n",
    "def make_batch(bs):\n",
    "    import random\n",
    "    return [random.choice(PROMPT_POOL) for _ in range(bs)]\n",
    "\n",
    "\n",
    "def pct(vals, p, method=\"higher\"):\n",
    "    \"\"\"Nearest-rank percentile so results reflect actual runs.\"\"\"\n",
    "    arr = np.asarray(vals, float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if arr.size == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.percentile(arr, p, method=method))\n",
    "\n",
    "records = []\n",
    "\n",
    "if USE_GREEDY_SAMPLING:\n",
    "    sampling_opts   = dict(temperature=0.0, top_p=1.0)  # greedy, deterministic\n",
    "else:\n",
    "    sampling_opts   = dict(temperature=TEMPERATURE, top_p=TOP_P) # random\n",
    "\n",
    "for bs in BATCH_SIZES:\n",
    "    print(f\"Processing with the batch size (BS) = {bs:>3} ...\")\n",
    "\n",
    "    # Per-run logs\n",
    "    first_latencies = []            # seconds\n",
    "    total_latencies = []            # seconds\n",
    "    sec_per_token_runs = []         # seconds / token\n",
    "    itl_sec_per_token_runs = []     # seconds / token (beyond first tokens)\n",
    "\n",
    "    # Aggregates for sustained throughput\n",
    "    sum_tokens_total = 0\n",
    "    sum_elapsed_total = 0.0\n",
    "\n",
    "    for r in range(RUNS_PER_SIZE + WARMUP_RUNS):\n",
    "        prompts = make_batch(bs)\n",
    "\n",
    "        # TTFT (first token only)\n",
    "        t_first, _, _, _ = timed_generate(\n",
    "            prompts,\n",
    "            SamplingParams(**sampling_opts, max_tokens=1),\n",
    "            as_chat=True,\n",
    "            return_counts=True\n",
    "        )\n",
    "\n",
    "        # Full answer\n",
    "        t_total, gen_tokens, counts, outs = timed_generate(\n",
    "            prompts,\n",
    "            SamplingParams(**sampling_opts, max_tokens=MAX_TOKENS_FULL),\n",
    "            as_chat=True,\n",
    "            return_counts=True\n",
    "        )\n",
    "\n",
    "        # Warmup discard\n",
    "        if r < WARMUP_RUNS:\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        # Track latencies\n",
    "        first_latencies.append(t_first)\n",
    "        total_latencies.append(t_total)\n",
    "\n",
    "        # Sustained throughput aggregates\n",
    "        sum_tokens_total += gen_tokens\n",
    "        sum_elapsed_total += t_total\n",
    "\n",
    "        # Throughput percentiles (via sec/token)\n",
    "        if gen_tokens > 0:\n",
    "            sec_per_token_runs.append(t_total / gen_tokens)\n",
    "\n",
    "        # ITL: exclude one first token per *active* sequence\n",
    "        post_first_tokens = sum((c - 1) for c in counts if c > 0)\n",
    "        itl_den = max(post_first_tokens, 1)\n",
    "        itl_sec_per_token_runs.append(max(t_total - t_first, 0.0) / itl_den)\n",
    "\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Metrics ---\n",
    "\n",
    "    # Latency percentiles (end-to-end)\n",
    "    p50_ttft_s    = pct(first_latencies, 50)\n",
    "    p90_ttft_s    = pct(first_latencies, 90)\n",
    "    p50_latency_s = pct(total_latencies, 50)\n",
    "    p90_latency_s = pct(total_latencies, 90)\n",
    "\n",
    "    # Sustained throughput = total generated tokens / total wall time\n",
    "    throughput_sustained_tok_s = (sum_tokens_total / sum_elapsed_total) if sum_elapsed_total > 0 else float(\"nan\")\n",
    "\n",
    "    # Throughput percentiles (invert sec/token percentiles)\n",
    "    p50_thru_tok_s = (1.0 / pct(sec_per_token_runs, 50)) if sec_per_token_runs else float(\"nan\")\n",
    "    p90_thru_tok_s = (1.0 / pct(sec_per_token_runs, 90)) if sec_per_token_runs else float(\"nan\")\n",
    "\n",
    "    # ITL percentiles (ms/token)\n",
    "    itl_p50_ms_per_tok = 1000.0 * pct(itl_sec_per_token_runs, 50) if itl_sec_per_token_runs else float(\"nan\")\n",
    "    itl_p90_ms_per_tok = 1000.0 * pct(itl_sec_per_token_runs, 90) if itl_sec_per_token_runs else float(\"nan\")\n",
    "\n",
    "    records.append(dict(\n",
    "        batch_size                 = bs,\n",
    "        p50_ttft_s                 = p50_ttft_s,\n",
    "        p90_ttft_s                 = p90_ttft_s,\n",
    "        p50_latency_s              = p50_latency_s,\n",
    "        p90_latency_s              = p90_latency_s,\n",
    "        throughput_sustained_tok_s = throughput_sustained_tok_s,\n",
    "        throughput_p50_tok_s       = p50_thru_tok_s,\n",
    "        throughput_p90_tok_s       = p90_thru_tok_s,\n",
    "        itl_p50_ms_per_tok         = itl_p50_ms_per_tok,\n",
    "        itl_p90_ms_per_tok         = itl_p90_ms_per_tok,\n",
    "        runs                       = RUNS_PER_SIZE\n",
    "    ))\n",
    "\n",
    "    print(\n",
    "        f\"BS={bs:>3} | p50_lat={p50_latency_s:.3f}s | p90_lat={p90_latency_s:.3f}s | \"\n",
    "        f\"sustained={throughput_sustained_tok_s:.1f} tok/s | p50_thru={p50_thru_tok_s:.1f} | p90_thru={p90_thru_tok_s:.1f} | \"\n",
    "        f\"ITL p50={itl_p50_ms_per_tok:.1f} ms/tok | ITL p90={itl_p90_ms_per_tok:.1f} ms/tok\"\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame.from_records(records).sort_values(\"batch_size\")\n",
    "display(df)\n",
    "\n",
    "df.to_csv(\"batch_benchmark.csv\", index=False)\n",
    "print(\"✅ Benchmark finished – saved to batch_benchmark.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e73fdd-59a8-4d27-9895-d36383c073bb",
   "metadata": {},
   "source": [
    "### Interpreting our benchmarks\n",
    "\n",
    "Now that we've run our performance sweep, let’s dive into what the results actually mean — and what insights we can extract from them.\n",
    "\n",
    "Let's load the benchmark file if you are returning to the notebook. Ignore this step if you've just executed the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afcb06-d008-45f5-9f8c-4f73406d7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df from memory if present, otherwise read the CSV; validate columns.\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if \"df\" not in globals():\n",
    "    candidates = [\"batch_benchmark.csv\", \"/workspace/batch_benchmark.csv\"]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            df = pd.read_csv(p)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Couldn't find batch_benchmark.csv. Run the benchmark first.\")\n",
    "\n",
    "# Ensure types and order\n",
    "df[\"batch_size\"] = df[\"batch_size\"].astype(int)\n",
    "df = df.sort_values(\"batch_size\").reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5edef5-ec3f-40ed-ad8a-6b462e827072",
   "metadata": {},
   "source": [
    "#### Helper for consistent plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13008f21-c7fd-4836-a4a7-09c585bf1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small helper to keep plots consistent (one chart per figure).\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_lines(x, series_list, title, xlabel, ylabel, ylog=False, xticks=None):\n",
    "    plt.figure()\n",
    "    for y, label, marker in series_list:\n",
    "        plt.plot(x, y, marker=marker, label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    if xticks is not None:\n",
    "        plt.xticks(xticks)\n",
    "    if ylog:\n",
    "        plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdcc98e-8922-4546-b438-6f97a5d4b0af",
   "metadata": {},
   "source": [
    "#### Throughput vs batch size (sustained, p50, p90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80d0b5-fca4-4399-aa70-277dc2aeba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"batch_size\"].tolist()\n",
    "\n",
    "plot_lines(\n",
    "    x,\n",
    "    [\n",
    "        (df[\"throughput_sustained_tok_s\"], \"Sustained\", \"o\"),\n",
    "        (df[\"throughput_p50_tok_s\"],       \"p50\",       \"s\"),\n",
    "        (df[\"throughput_p90_tok_s\"],       \"p90\",       \"^\"),\n",
    "    ],\n",
    "    title=\"Throughput vs Batch Size (tokens/sec)\",\n",
    "    xlabel=\"Batch size\",\n",
    "    ylabel=\"Tokens / second\",\n",
    "    ylog=False,\n",
    "    xticks=x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553c7c4-dcf0-4db9-8fe1-9cffd0ea27ae",
   "metadata": {},
   "source": [
    "#### Total latency vs batch size (p50, p99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b1946-b57b-4de6-927e-23bd9dac464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"batch_size\"].tolist()\n",
    "\n",
    "plot_lines(\n",
    "    x,\n",
    "    [\n",
    "        (df[\"p50_latency_s\"], \"p50 total latency\", \"o\"),\n",
    "        (df[\"p90_latency_s\"], \"p90 total latency\", \"s\"),\n",
    "    ],\n",
    "    title=\"Total Latency vs Batch Size\",\n",
    "    xlabel=\"Batch size\",\n",
    "    ylabel=\"Seconds\",\n",
    "    ylog=True,      # latencies can span orders of magnitude\n",
    "    xticks=x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a89e9a-75df-4f8d-a127-352e1a9fd5b7",
   "metadata": {},
   "source": [
    "#### Time to First Token (TTFT) vs batch size (p50, p99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785eeaf6-d7e9-4e0b-95c7-978e17619ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"batch_size\"].tolist()\n",
    "\n",
    "plot_lines(\n",
    "    x,\n",
    "    [\n",
    "        (df[\"p50_ttft_s\"], \"p50 TTFT\", \"o\"),\n",
    "        (df[\"p90_ttft_s\"], \"p90 TTFT\", \"s\"),\n",
    "    ],\n",
    "    title=\"Time to First Token (TTFT) vs Batch Size\",\n",
    "    xlabel=\"Batch size\",\n",
    "    ylabel=\"Seconds\",\n",
    "    ylog=True,\n",
    "    xticks=x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423ba58-b4c5-46c4-8b5f-10eb16461529",
   "metadata": {},
   "source": [
    "#### First-token vs total latency across batch sizes (p50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7db988-9b2a-4ead-a1cd-277c7e31d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort & extract\n",
    "x = df[\"batch_size\"].tolist()\n",
    "\n",
    "ttft_p50 = df[\"p50_ttft_s\"].tolist()\n",
    "ttft_p90 = df[\"p90_ttft_s\"].tolist()\n",
    "lat_p50  = df[\"p50_latency_s\"].tolist()\n",
    "lat_p90  = df[\"p90_latency_s\"].tolist()\n",
    "\n",
    "# Single figure with four series\n",
    "plt.figure()\n",
    "plt.plot(x, ttft_p50, marker=\"o\", label=\"TTFT (p50)\")\n",
    "plt.plot(x, ttft_p90, marker=\"^\", label=\"TTFT (p90)\")\n",
    "plt.plot(x, lat_p50,  marker=\"s\", label=\"Total latency (p50)\")\n",
    "plt.plot(x, lat_p90,  marker=\"x\", label=\"Total latency (p90)\")\n",
    "\n",
    "plt.title(\"First-Token vs Total Latency — p50 & p90\")\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Seconds\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.xticks(x)\n",
    "plt.yscale(\"log\")  # latency scales fast with batch size; log makes it readable\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c965f9a-745b-4abb-940b-d45d35495c83",
   "metadata": {},
   "source": [
    "#### Inter-Token Latency (ITL) vs batch size (p50, p90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdcfac7-6528-4d35-a487-8c89c147c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"batch_size\"].tolist()\n",
    "\n",
    "plot_lines(\n",
    "    x,\n",
    "    [\n",
    "        (df[\"itl_p50_ms_per_tok\"], \"ITL p50\", \"o\"),\n",
    "        (df[\"itl_p90_ms_per_tok\"], \"ITL p90\", \"s\"),\n",
    "    ],\n",
    "    title=\"Inter-Token Latency (ITL) vs Batch Size\",\n",
    "    xlabel=\"Batch size\",\n",
    "    ylabel=\"Milliseconds / token\",\n",
    "    ylog=True,\n",
    "    xticks=x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4c4ee-cfdb-458c-9825-53b5a4f44be8",
   "metadata": {},
   "source": [
    "#### Throughput vs latency trade-off (capacity vs responsiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861ab6a-621d-459d-a767-a7921f4e6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = df[\"p50_latency_s\"]\n",
    "y = df[\"throughput_sustained_tok_s\"]\n",
    "labels = df[\"batch_size\"].tolist()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, marker=\"o\")\n",
    "for xi, yi, lbl in zip(x, y, labels):\n",
    "    plt.annotate(str(lbl), (xi, yi), textcoords=\"offset points\", xytext=(5,5))\n",
    "plt.title(\"Throughput vs p50 Latency\")\n",
    "plt.xlabel(\"p50 total latency (s)\")\n",
    "plt.ylabel(\"Sustained throughput (tok/s)\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9c555-984c-4c69-ac71-6eb94dc03156",
   "metadata": {},
   "source": [
    "#### Latency vs Throughput with knee point (p50 latency vs sustained throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c7219-e592-41ce-93c3-50e8f90c6895",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_COL = \"p50_latency_s\"              # latency column\n",
    "T_COL = \"throughput_sustained_tok_s\" # throughput column\n",
    "\n",
    "needed = [\"batch_size\", L_COL, T_COL]\n",
    "missing = [c for c in needed if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}. Re-run the benchmark with those fields.\")\n",
    "\n",
    "# Sort and extract arrays\n",
    "df = df.sort_values(\"batch_size\").reset_index(drop=True)\n",
    "bs  = df[\"batch_size\"].to_numpy(dtype=int)\n",
    "lat = df[L_COL].to_numpy(dtype=float)\n",
    "thr = df[T_COL].to_numpy(dtype=float)\n",
    "\n",
    "# Normalize latency to [0,1]; normalize throughput and invert to [0,1]\n",
    "lat_n   = lat / np.max(lat)                    # higher is worse\n",
    "thr_inv = 1.0 - (thr / np.max(thr))            # invert so higher is worse\n",
    "\n",
    "# Gap between the two normalized curves → knee where gap is smallest\n",
    "gap    = np.abs(lat_n - thr_inv)\n",
    "best_i = int(np.argmin(gap))\n",
    "\n",
    "# Plot in your requested style\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(bs, lat_n,   'o-', label=f'{L_COL} (norm)')\n",
    "plt.plot(bs, thr_inv, '^--', label=f'{T_COL} (inverted norm)')\n",
    "plt.scatter(bs[best_i], lat_n[best_i], c='red', s=120,\n",
    "            label=f'sweet spot ≈ BS {bs[best_i]}')\n",
    "plt.xlabel(\"Batch size\"); plt.ylabel(\"Normalised metric (0-1)\")\n",
    "plt.title(\"Latency vs Throughput – knee point\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Knee ~ BS={bs[best_i]} | \"\n",
    "    f\"latency={lat[best_i]:.3f}s | throughput={thr[best_i]:.1f} tok/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6bfe86-762c-41d6-a2d2-f75d50cfa023",
   "metadata": {},
   "source": [
    "#### Finding the sweet spot based on UX budget\n",
    "\n",
    "Choosing the optimal batch size isn’t just about throughput — it’s about respecting your users’ **latency expectations**.\n",
    "\n",
    "This section helps you find the best-performing batch size that stays within a given **latency budget**, defined in seconds. You can adjust the value of `latency_budget` depending on your application’s needs (e.g. 2.5s for chat, 5s for summarization, etc.).\n",
    "\n",
    "What this code does:\n",
    "\n",
    "- Filters all benchmark results to include only configurations where latency is **below your target threshold**\n",
    "- Then selects the **batch size with the highest throughput** from the remaining options\n",
    "- Runs this selection twice:\n",
    "  - Once using **p50 latency** (typical case)\n",
    "  - Once using **p99 latency** (worst-case tail latency)\n",
    "\n",
    "Try changing the `latency_budget` value and observe how the recommended batch size shifts depending on the metric used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793b4d4-177c-4ea1-a755-0ace9a779098",
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_budget = 7  # seconds\n",
    "\n",
    "filtered_p50 = df[df[\"p50_latency_s\"] <= latency_budget]\n",
    "filtered_p90 = df[df[\"p90_latency_s\"] <= latency_budget]\n",
    "\n",
    "if not filtered_p50.empty:\n",
    "    best_row_p50 = filtered_p50.loc[filtered_p50[\"throughput_sustained_tok_s\"].idxmax()]\n",
    "    print(f\"✅ Recommended: batch_size={int(best_row_p50.batch_size)}  \"\n",
    "          f\"⇒  p50 latency = {best_row_p50.p50_latency_s:.2f}s, \"\n",
    "          f\"throughput = {best_row_p50.throughput_sustained_tok_s:.0f} tok/s\")\n",
    "else:\n",
    "    print(f\"❌ No configuration meets the latency budget of {latency_budget:.1f}s\")\n",
    "\n",
    "if not filtered_p90.empty:\n",
    "    best_row_p90 = filtered_p90.loc[filtered_p90[\"throughput_sustained_tok_s\"].idxmax()]\n",
    "    print(f\"✅ Recommended: batch_size={int(best_row_p90.batch_size)}  \"\n",
    "          f\"⇒  p90 latency = {best_row_p90.p90_latency_s:.2f}s, \"\n",
    "          f\"throughput = {best_row_p90.throughput_sustained_tok_s:.0f} tok/s\")\n",
    "else:\n",
    "    print(f\"❌ No configuration meets the latency budget of {latency_budget:.1f}s based on p90 latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57695205-5a25-4257-b63a-79fb7548b584",
   "metadata": {},
   "source": [
    "### Asessing impact of other parameters\n",
    "\n",
    "So far, we’ve seen that **batch size** directly affects latency and throughput. But batch size isn’t the only factor that matters.\n",
    "\n",
    "Let’s now explore how **other inference parameters**, starting with **output sequence length**, influence performance.\n",
    "\n",
    "#### Sequence Length\n",
    "\n",
    "The **sequence length** `max_tokens` parameter defines how many tokens the model is allowed to generate per prompt.\n",
    "\n",
    "In practice:\n",
    "\n",
    "- Short outputs (e.g. 32 tokens) return quickly\n",
    "- Long outputs (e.g. 512 tokens) take significantly more time, especially at high batch sizes\n",
    "\n",
    "This is because while **prefill cost is fixed**, the **decode phase scales linearly** with the number of tokens generated — we will look into that in future tutorials!\n",
    "\n",
    "What this experiment does:\n",
    "\n",
    "- Uses a fixed batch size (`batch = 32`)\n",
    "- Varies the `max_tokens` cap from 32 to 256\n",
    "- Measures **mean** and **p99 latency** across 5 runs for each setting\n",
    "\n",
    "> Note: This experiment can take a few minutes — longer sequence lengths at high batch size are compute-intensive.\n",
    "\n",
    "Let’s visualize how increasing the output length impacts both average latency and tail latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add23d47-4411-445d-8406-faaee29c4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency vs Generated Length (p50 & p90) — batch fixed\n",
    "\n",
    "import gc, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from vllm import SamplingParams\n",
    "\n",
    "seq_lengths = [32, 64, 128, 256]\n",
    "BATCH       = 32\n",
    "RUNS        = 20   # effective samples per point (after warmup)\n",
    "WARMUP      = 2\n",
    "\n",
    "# Non-greedy (realistic) sampling\n",
    "NON_GREEDY = dict(temperature=0.7, top_p=0.9)\n",
    "\n",
    "lat_p50s, lat_p90s = [], []\n",
    "\n",
    "for mt in seq_lengths:\n",
    "    sp = SamplingParams(max_tokens=mt, **NON_GREEDY)\n",
    "    lats = []\n",
    "    for r in range(RUNS + WARMUP):\n",
    "        prompts = [\"Summer is\"] * BATCH\n",
    "        t_total, _, _, _ = timed_generate(prompts, sp, as_chat=True, return_counts=True)\n",
    "        if r < WARMUP:\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            continue\n",
    "        lats.append(t_total)\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # Percentiles on observed latencies (nearest-rank style)\n",
    "    p50 = float(np.percentile(lats, 50, method=\"higher\")) if lats else float(\"nan\")\n",
    "    p90 = float(np.percentile(lats, 90, method=\"higher\")) if lats else float(\"nan\")\n",
    "    lat_p50s.append(p50)\n",
    "    lat_p90s.append(p90)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(seq_lengths, lat_p50s, marker='o', label='p50 latency')\n",
    "plt.plot(seq_lengths, lat_p90s, marker='s', label='p90 latency')\n",
    "plt.xlabel('max_tokens'); plt.ylabel('Latency (s)'); plt.title('Latency vs Generated Length (Batch = 32)')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6201183-82ac-4dea-bb34-5b7768ce9385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
